{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "\n",
    "import mne\n",
    "from mne.datasets import visual_92_categories\n",
    "from mne.io import concatenate_raws, read_raw_fif\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "data_path = visual_92_categories.data_path()\n",
    "\n",
    "# Define stimulus - trigger mapping\n",
    "fname = data_path / \"visual_stimuli.csv\"\n",
    "conds = read_csv(fname)\n",
    "print(conds.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_trigger = 24\n",
    "conds = conds[:max_trigger]  # take only the first 24 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = []\n",
    "for c in conds.values:\n",
    "    cond_tags = list(c[:2])\n",
    "    cond_tags += [\n",
    "        (\"not-\" if i == 0 else \"\") + conds.columns[k] for k, i in enumerate(c[2:], 2)\n",
    "    ]\n",
    "    conditions.append(\"/\".join(map(str, cond_tags)))\n",
    "print(conditions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_id = dict(zip(conditions, conds.trigger + 1))\n",
    "event_id[\"2/human bodypart/human/not-face/animal/natural\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 4  # 4 for full data (use less to speed up computations)\n",
    "fnames = [data_path / f\"sample_subject_{b}_tsss_mc.fif\" for b in range(n_runs)]\n",
    "raws = [\n",
    "    read_raw_fif(fname, verbose=\"error\", on_split_missing=\"ignore\") for fname in fnames\n",
    "]  # ignore filename warnings\n",
    "raw = concatenate_raws(raws)\n",
    "\n",
    "events = mne.find_events(raw, min_duration=0.002)\n",
    "\n",
    "events = events[events[:, 2] <= max_trigger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picks = mne.pick_types(raw.info, meg=True)\n",
    "epochs = mne.Epochs(\n",
    "    raw,\n",
    "    events=events,\n",
    "    event_id=event_id,\n",
    "    baseline=None,\n",
    "    picks=picks,\n",
    "    tmin=-0.1,\n",
    "    tmax=0.400,\n",
    "    preload=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs[\"face\"].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs[\"not-face\"].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs.get_data().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs[\"2/human bodypart/human/not-face/animal/natural\"].get_data().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evoked = [epochs[k].average().get_data() for k in event_id]\n",
    "evoked = np.array(evoked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evoked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "def round_to_zero_if_close(value):\n",
    "    \"\"\"\n",
    "    Rounds a float value to zero if it's close enough (within 1e-15).\n",
    "\n",
    "    Args:\n",
    "        value (float): The float value to round to zero.\n",
    "\n",
    "    Returns:\n",
    "        float: The original value rounded to zero if it's close enough, otherwise the original value.\n",
    "    \"\"\"\n",
    "    if abs(value) < 1e-15:\n",
    "        return 0\n",
    "    return value\n",
    "\n",
    "def calculate_rdm(activity, num_conditions, calculate_absolute=False):\n",
    "    \"\"\"\n",
    "    Calculates the Representational Dissimilarity Matrix (RDM) for one Artificial Neural Network layer\n",
    "    activations or for a brain region.\n",
    "\n",
    "    Args:\n",
    "        activity (np.ndarray): The model layer/brain region activity patterns.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The activations RDM. The shape is [num_conditions, num_conditions].\n",
    "    \"\"\"\n",
    "    rdm = np.zeros((num_conditions, num_conditions), dtype=np.float64)\n",
    "\n",
    "    for i in range(num_conditions):\n",
    "        for j in range(i + 1, num_conditions):\n",
    "            r = pearsonr(activity[i], activity[j])[0]\n",
    "            value = 1 - np.abs(r) if calculate_absolute else 1 - r\n",
    "            rdm[i, j] = round_to_zero_if_close(value)\n",
    "\n",
    "    return rdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from tqdm import tqdm\n",
    "def brain_rdm_movie(brain_activity: Dict[str, np.ndarray], num_conditions: int, t_start: int = 0, t_end: int = 501) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute a movie of Representational Dissimilarity Matrices (RDMs) for brain activity over time.\n",
    "\n",
    "    This function calculates a sequence of RDMs over time from the brain activity data provided.\n",
    "    Each RDM in the movie represents the dissimilarity between conditions at a specific time point.\n",
    "\n",
    "    Args:\n",
    "        brain_activity (dict): A dictionary containing the brain regions/sensors activity patterns.\n",
    "        t_start (int, optional): The starting time point for the RDM movie, default is 0.\n",
    "        t_end (int, optional): The ending time point for the RDM movie, default is 501.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 4D array representing the RDM movie. Shape: [num_brain_elements, rdm_movie_length, num_conditions, num_conditions].\n",
    "\n",
    "    Note:\n",
    "    - The function computes RDMs over a specified time range for each brain element in the input data.\n",
    "    - The resulting RDM movie captures temporal changes in dissimilarity patterns between conditions.\n",
    "    \"\"\"\n",
    "    rdm_movie_length = t_end - t_start\n",
    "    num_brain_elements = len(brain_activity)\n",
    "    rdms = np.zeros((num_brain_elements, rdm_movie_length, num_conditions, num_conditions), dtype=np.float64)\n",
    "    for brain_element_id, (_, brain_element_activity) in tqdm(enumerate(brain_activity.items()), desc=\"Calculating brain RDMs movie\", total=num_brain_elements):\n",
    "        for t in range(rdm_movie_length):\n",
    "            segment_activity = brain_element_activity[:, :, t_start + t]\n",
    "            rdms[brain_element_id, t] = calculate_rdm(segment_activity, num_conditions)\n",
    "\n",
    "    return rdms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_activity = {\"brain\": evoked}\n",
    "num_conditions = len(event_id)\n",
    "rdm_movie = brain_rdm_movie(brain_activity, num_conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdm_movie.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdm_movie = rdm_movie[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdm_movie.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "time_interval = 50\n",
    "num_timepoints = rdm_movie.shape[0]\n",
    "total_time = num_timepoints \n",
    "\n",
    "indices = np.arange(0, total_time, time_interval)\n",
    "\n",
    "num_plots = len(indices)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=num_plots, figsize=(50, 5))\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    ax = axes[i]\n",
    "    ax.imshow(rdm_movie[idx], cmap='viridis', vmin=0, vmax=np.max(rdm_movie), aspect='auto')\n",
    "    ax.set_title(f'Time: {idx} s')\n",
    "    ax.set_xlabel('Condition')\n",
    "    ax.set_ylabel('Condition')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "stimuli = {\n",
    "    '0/human bodypart/human/not-face/animal/natural': 1,\n",
    "    '1/human bodypart/human/not-face/animal/natural': 2,\n",
    "    '2/human bodypart/human/not-face/animal/natural': 3,\n",
    "    '3/human bodypart/human/not-face/animal/natural': 4,\n",
    "    '4/human bodypart/human/not-face/animal/natural': 5,\n",
    "    '5/human bodypart/human/not-face/animal/natural': 6,\n",
    "    '6/human bodypart/human/not-face/animal/natural': 7,\n",
    "    '7/human bodypart/human/not-face/animal/natural': 8,\n",
    "    '8/human bodypart/human/not-face/animal/natural': 9,\n",
    "    '9/human bodypart/human/not-face/animal/natural': 10,\n",
    "    '10/human bodypart/human/not-face/animal/natural': 11,\n",
    "    '11/human bodypart/human/not-face/animal/natural': 12,\n",
    "    '12/human face/human/face/animal/natural': 13,\n",
    "    '13/human face/human/face/animal/natural': 14,\n",
    "    '14/human face/human/face/animal/natural': 15,\n",
    "    '15/human face/human/face/animal/natural': 16,\n",
    "    '16/human face/human/face/animal/natural': 17,\n",
    "    '17/human face/human/face/animal/natural': 18,\n",
    "    '18/human face/human/face/animal/natural': 19,\n",
    "    '19/human face/human/face/animal/natural': 20,\n",
    "    '20/human face/human/face/animal/natural': 21,\n",
    "    '21/human face/human/face/animal/natural': 22,\n",
    "    '22/human face/human/face/animal/natural': 23,\n",
    "    '23/human face/human/face/animal/natural': 24\n",
    "}\n",
    "\n",
    "stimuli_df = pd.DataFrame(list(stimuli.items()), columns=['stimulus', 'id'])\n",
    "\n",
    "num_stimuli = len(stimuli)\n",
    "rdm_faces = np.zeros((num_stimuli, num_stimuli))\n",
    "rdm_animate = np.zeros((num_stimuli, num_stimuli))\n",
    "\n",
    "face_indices = [i for i in range(num_stimuli) if 'human face' in stimuli_df['stimulus'][i]]\n",
    "non_face_indices = [i for i in range(num_stimuli) if 'not-face' in stimuli_df['stimulus'][i]]\n",
    "\n",
    "animate_indices = [i for i in range(num_stimuli) if 'animal' in stimuli_df['stimulus'][i]]\n",
    "non_animate_indices = [i for i in range(num_stimuli) if 'not-face' in stimuli_df['stimulus'][i]]\n",
    "\n",
    "for i in range(num_stimuli):\n",
    "    for j in range(num_stimuli):\n",
    "        if i == j:\n",
    "            rdm_faces[i, j] = 0  \n",
    "            rdm_animate[i, j] = 0\n",
    "        else:\n",
    "            # Dissimilarity for face vs. non-face\n",
    "            if (i in face_indices and j in face_indices) or (i in non_face_indices and j in non_face_indices):\n",
    "                rdm_faces[i, j] = 1  # Similar\n",
    "            else:\n",
    "                rdm_faces[i, j] = 2  # Dissimilar\n",
    "\n",
    "            # Dissimilarity for animate vs. non-animate\n",
    "            if (i in animate_indices and j in animate_indices) or (i in non_animate_indices and j in non_animate_indices):\n",
    "                rdm_animate[i, j] = 1  # Similar\n",
    "            else:\n",
    "                rdm_animate[i, j] = 2  # Dissimilar\n",
    "\n",
    "print(\"RDM for Faces vs Non-Faces:\\n\", rdm_faces)\n",
    "print(\"RDM for Animate vs Non-Animate:\\n\", rdm_animate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# RDM for Faces vs Non-Faces\n",
    "cax1 = axs[0].imshow(rdm_faces, cmap='viridis', vmin=0, vmax=2)\n",
    "axs[0].set_title('RDM: Faces vs Non-Faces')\n",
    "axs[0].set_xticks(np.arange(num_stimuli))\n",
    "axs[0].set_yticks(np.arange(num_stimuli))\n",
    "axs[0].set_xticklabels([], rotation=90)\n",
    "axs[0].set_yticklabels([])\n",
    "fig.colorbar(cax1, ax=axs[0])\n",
    "\n",
    "# RDM for Animate vs Non-Animate\n",
    "cax2 = axs[1].imshow(rdm_animate, cmap='viridis', vmin=0, vmax=2)\n",
    "axs[1].set_title('RDM: Animate vs Non-Animate')\n",
    "axs[1].set_xticks(np.arange(num_stimuli))\n",
    "axs[1].set_yticks(np.arange(num_stimuli))\n",
    "axs[1].set_xticklabels([], rotation=90)\n",
    "axs[1].set_yticklabels([])\n",
    "fig.colorbar(cax2, ax=axs[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rsa(rdm1_vector: np.array, rdm2_vector: np.array, permutation: bool, iter: int = 1000) -> np.array:\n",
    "    \"\"\"\n",
    "    Compute the similarity between two RDMs.\n",
    "\n",
    "    Args:\n",
    "        rdm1_vector (np.array): The first RDM.\n",
    "        rdm2_vector (np.array): The second RDM.\n",
    "        permutation (bool): Whether to use permutation tests or not.\n",
    "\n",
    "    Returns:\n",
    "        rp (np.array): The similarity measure and p-value.\n",
    "\n",
    "    \"\"\"\n",
    "    if len(rdm1_vector) != len(rdm2_vector):\n",
    "        raise ValueError(\"The two RDMs must have the same length.\")\n",
    "    rp = np.array(pearsonr(rdm1_vector, rdm2_vector))\n",
    "\n",
    "\n",
    "    return rp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_rdm_vectors( rdms: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Transform 2D RDMs into 1D vectors using the upper triangle.\n",
    "\n",
    "    Args:\n",
    "        rdms (np.array): The RDMs. Shape: [num_rdms, num_conditions, num_conditions].\n",
    "\n",
    "    Returns:\n",
    "        rdm_vectors (np.array): The RDM vectors. Shape: [num_rdms, num_conditions * (num_conditions - 1) / 2].\n",
    "\n",
    "    \"\"\"\n",
    "    num_rdms = rdms.shape[0]\n",
    "    num_conditions = rdms.shape[-1]\n",
    "    rdm_vectors = np.zeros((num_rdms, int(num_conditions * (num_conditions - 1) / 2)))\n",
    "    for i in range(num_rdms):\n",
    "        rdm_vectors[i] = rdms[i][np.triu_indices(num_conditions, k=1)]\n",
    "    return rdm_vectors\n",
    "\n",
    "def rsa( meg_rdm: np.array, model_rdm: np.array, permutation: bool = False, iter: int = 1000) -> np.array:\n",
    "    \"\"\"\n",
    "    Compute the similarity between MEG RDMs and model RDMs.\n",
    "\n",
    "    Args:\n",
    "        meg_rdm (np.array): A 4D MEG RDM movie. Shape: [num_brain_elements, num_rdms, num_conditions, num_conditions].\n",
    "        model_rdm (np.array): The model RDMs. Shape: [num_layers, num_conditions, num_conditions]\n",
    "        permutation (bool): Whether to use permutation tests or not.\n",
    "\n",
    "    Returns:\n",
    "        similarity (np.array): The similarity measure and p-value. Shape: [num_brain_elements, num_rdms, num_layers, 2].\n",
    "\n",
    "    \"\"\"\n",
    "    if meg_rdm.shape[2] != model_rdm.shape[1]:\n",
    "        raise ValueError(\"The two RDMs must have the same length.\")\n",
    "\n",
    "\n",
    "    num_brain_elements = meg_rdm.shape[0]\n",
    "    rdm_length = meg_rdm.shape[1]\n",
    "    num_layers = model_rdm.shape[0]\n",
    "    meg_rdm_vector = np.zeros((num_brain_elements, rdm_length, int(meg_rdm.shape[-1] * (meg_rdm.shape[-1] - 1) / 2)))\n",
    "    for i in range(num_brain_elements):\n",
    "        meg_rdm_vector[i] =  _get_rdm_vectors(meg_rdm[i])\n",
    "    model_rdm_vector = _get_rdm_vectors(model_rdm) \n",
    "    del meg_rdm, model_rdm\n",
    "    similarity = np.zeros((num_brain_elements, rdm_length, num_layers, 2))\n",
    "\n",
    "    for i in tqdm(range(num_brain_elements), desc=\"Computing RSA\"):\n",
    "        for j in range(rdm_length):\n",
    "            for k in range(num_layers):\n",
    "                similarity[i, j, k] = calculate_rsa(meg_rdm_vector[i, j], model_rdm_vector[k], permutation)\n",
    "    return similarity\n",
    "\n",
    "def score( meg_rdm: np.array, model_rdm: np.array, parrallel: bool = True, permutation: bool = False, iter: int = 1000) -> np.array:\n",
    "    \"\"\"\n",
    "    Compute the similarity between MEG RDMs and model RDMs.\n",
    "\n",
    "    Args:\n",
    "        meg_rdm (np.array): A 4D MEG RDM movie. Shape: [num_brain_elements, num_rdms, num_conditions, num_conditions].\n",
    "        model_rdm (np.array): The model RDMs. Shape: [num_layers, num_conditions, num_conditions]\n",
    "        parrallel (bool): Whether to use parallel computation or not.\n",
    "        permutation (bool): Whether to use permutation tests or not.\n",
    "        iter (int): The number of iterations for permutation tests.\n",
    "\n",
    "    Returns:\n",
    "        similarity (np.array): The similarity measure and p-value. Shape: [num_brain_elements, num_rdms, num_layers, 2].\n",
    "\n",
    "    \"\"\"\n",
    "    return rsa(meg_rdm, model_rdm, permutation, iter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdm_faces = np.reshape(rdm_faces, (1, 24, 24))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdm_movie = np.reshape(rdm_movie, (1, 501, 24, 24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity=score(rdm_movie, rdm_faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_values = similarity[0, :, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the time range from -100 ms to 400 ms\n",
    "time = np.linspace(-100, 400, num=501)  # Creates an array of 501 values from -100 to 400\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot with improved aesthetics\n",
    "plt.plot(time, similarity_values, color='blue', linewidth=2, label='Similarity Values')\n",
    "plt.axhline(0, color='red', linestyle='--', linewidth=1, label='Zero Reference')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Similarity Values Over Time', fontsize=16)\n",
    "plt.xlabel('Time (ms)', fontsize=14)\n",
    "plt.ylabel('Similarity Values', fontsize=14)\n",
    "\n",
    "# Customizing ticks\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Adding a grid\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Setting limits\n",
    "plt.xlim(-100, 400)\n",
    "plt.ylim(min(similarity_values) - 0.1, max(similarity_values) + 0.1)\n",
    "\n",
    "# Adding a legend\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Saving the figure in a high-resolution format\n",
    "plt.tight_layout()\n",
    "plt.savefig('similarity_plot.png', dpi=300, bbox_inches='tight')  # Save as PNG with high DPI\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "# Load the pretrained AlexNet model\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "alexnet.eval()\n",
    "\n",
    "# If you want to move the model to GPU (if available)\n",
    "if torch.cuda.is_available():\n",
    "    alexnet = alexnet.to('cuda')\n",
    "\n",
    "# Now you can use alexnet for inference or further fine-tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.modules as mod\n",
    "\n",
    "# Global variable to store activations\n",
    "activations = {}\n",
    "\n",
    "def get_activation_hook(name: str):\n",
    "    \"\"\"\n",
    "    Create a hook to capture the activation of a specific layer in the network.\n",
    "\n",
    "    Parameters:\n",
    "        name (str): A name to identify the activation tensor in the activations dictionary.\n",
    "\n",
    "    Returns:\n",
    "        Callable: A hook function that can be registered to capture activations.\n",
    "    \"\"\"\n",
    "    def hook(module, input, output):\n",
    "        \"\"\"\n",
    "        Hook function to capture the activation tensor.\n",
    "\n",
    "        This function detaches the tensor from the computation graph, converts it to a NumPy array,\n",
    "        and reshapes it for further analysis.\n",
    "\n",
    "        Parameters:\n",
    "            module (torch.nn.Module): The layer to which the hook is attached.\n",
    "            input (tuple): Input tensors to the layer.\n",
    "            output (torch.Tensor): The output tensor of the layer.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Detach the tensor from the computation graph, convert to NumPy, and reshape\n",
    "        activation = output.detach().cpu().numpy().reshape(output.size(0), -1)\n",
    "        activations[name] = activation\n",
    "\n",
    "    return hook\n",
    "\n",
    "from typing import Union,  Dict\n",
    "\n",
    "def model_activations(model: nn.Module,\n",
    "    data: torch.Tensor,\n",
    "    weights: Union[str, None] = None,\n",
    "    file_name: Union[str, None] = None,\n",
    "    transfer: bool = False\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Get a dictionary with the activations of selected layers for a given input tensor.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network architecture.\n",
    "        data (torch.Tensor): The input tensor to the network.\n",
    "        weights (str): The path to the trained model weights. If set to None, the model will use random weights.\n",
    "        file_name (str, optional): The name of the file to save the activations to.\n",
    "            This parameter is only used if 'save' is set to True.\n",
    "        transfer (bool, optional): Indicates whether transfer first layer.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with layer names as keys and their activations as values.\n",
    "              Each activation value is a numpy array of shape [n_cons, N_neurons_in_layer], where 'n_cons' is the\n",
    "              number of examples in the input tensor, and 'N_neurons_in_layer' is the number of neurons in the\n",
    "              corresponding layer.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    activations.clear()  # Clear previous activations\n",
    "\n",
    "    # Register hooks for selected layer types\n",
    "    for name, layer in model.named_modules():\n",
    "        layer.register_forward_hook(get_activation_hook(name))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(data)\n",
    "\n",
    "    return activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
